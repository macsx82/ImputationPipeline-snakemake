
#03/03/2020

Inizialization of the snakemake imputation pipeline!

* First rule will be the snp check using shapeit to align genotypes to the reference panel
* Second rule will be the update of misaligned data
* third rule will be the Phasing
* Finally we will submit the imputation

test command for rules 1-3:

```bash
snakemake --configfile config_chr_18.yaml --cores 8 
snakemake --configfile config_chr_21_22.yaml --cores 8 
```

We need to submit to the cluster:

```bash
# snakemake --configfile config_chr_18.yaml --cores 8 --cluster-config SGE_cluster.json --cluster "qsub -N ${pop}_${chr}_STEP6 -m ea -M {cluster.user_mail} -pe {cluster.parall_env} {threads} -o {config.output_folder}/\$JOB_ID_{config.pop}_{config.chr}.log -e {config.output_folder}/\$JOB_ID_STEP6_{config.pop}_{config.chr}.e -V -l h_vmem={cluster.mem} -q {cluster.queue}"
# snakemake --jobs 50 --configfile config_chr_18.yaml --cluster-config ~/scripts/pipelines/ImputationPipeline-snakemake/SGE_cluster.json --cluster "qsub -N ${pop}_${chr}_STEP6 -m ea -M {cluster.user_mail} -pe {cluster.parall_env} {threads} -o {config.output_folder}/\$JOB_ID_{config.pop}_{config.chr}.log -e {config.output_folder}/\$JOB_ID_STEP6_{config.pop}_{config.chr}.e -V -l h_vmem={cluster.mem} -q {cluster.queue}"
snakemake --jobs 50 --configfile config_chr_18.yaml --cluster-config ~/scripts/pipelines/ImputationPipeline-snakemake/SGE_cluster.json --cluster "qsub -N {config[pop]}_{config[chr]}_{rule} -m ea -M {cluster.user_mail} -pe {cluster.parall_env} {threads} -o {config[output_folder]}/\$JOB_ID_{config[pop]}_{config[chr]}.log -e {config[output_folder]}/\$JOB_ID_{config[pop]}_{config[chr]}.e -V -l h_vmem={cluster.mem} -q {cluster.queue}"
snakemake --jobs 50 --configfile config_chr_21_22.yaml --cluster-config ~/scripts/pipelines/ImputationPipeline-snakemake/SGE_cluster.json --cluster "qsub -N {config[pop]}_{wildcards.chrom}_{rule} -m ea -M {cluster.user_mail} -pe {cluster.parall_env} {threads} -o {config[output_folder]}/\$JOB_ID_{config[pop]}_{wildcards.chrom}.log -e {config[output_folder]}/\$JOB_ID_{config[pop]}_{wildcards.chrom}.e -V -l h_vmem={cluster.mem} -q {cluster.queue}"
```

qsub -N ${pop}_${chr}_STEP6 -m ea -M massimiliano.cocca@burlo.trieste.it -o ${outfolder}/\$JOB_ID_STEP6_${pop}_${chr}.log -e ${outfolder}/\$JOB_ID_STEP6_${pop}_${chr}.log.e -V -l h_vmem=${m} -q fast

snakemake -j 100 --cluster-config cluster.json --cluster "sbatch -A {cluster.account} --mem={cluster.mem} -t {cluster.time} -c {threads}"


Sample command:

```bash
snakemake -s ~/scripts/pipelines/ImputationPipeline-snakemake/Snakefile -p -r --configfile /home/cocca/analyses/test_imputation_20210604/config_test_2.yaml --cores 10 --keep-going
```

---
#05/03/2020

In order to fix the misaligned data before the imputation and phasing step, we check the genotypes against the reference panel selected.
We will flip all data with the "Strand" warning generated by shapeit, but we will remove data misaligned to INDELs and Multiallelic sites (removing ALL duplicated occurences)

```bash
fgrep -w "Strand" {input[0]} | awk 'length($9)==length($10) && $5!="D" && $5!="I"' | cut -f 4 | sort|uniq -u > {output.strand_rsid}
```

---
#05/05/2020
Create a wrapper to generate the config file

```bash
# snakemake -s ~/scripts/pipelines/ImputationPipeline-snakemake/Snakefile -p -r --jobs 50 --configfile /home/cocca/analyses/test_imputation/config_test_2.yaml --cluster-config ~/scripts/pipelines/ImputationPipeline-snakemake/SGE_cluster.json --cluster "qsub -N {config[cohort_name]}_{wildcards.chr}_{rule} -V -cwd -m ea -M {cluster.user_mail} -pe {cluster.parall_env} {threads} -o {config[paths][log_folder]}/\$JOB_ID_{config[cohort_name]}_{wildcards.chr}.log -e {config[paths][log_folder]}/\$JOB_ID_{config[cohort_name]}_{wildcards.chr}.e -V -l h_vmem={cluster.mem} -q {cluster.queue}"
snakemake -s ~/scripts/pipelines/ImputationPipeline-snakemake/Snakefile -p -r --jobs 50 --configfile /home/cocca/analyses/test_imputation_20210604/config_test_2.yaml --cluster-config ~/scripts/pipelines/ImputationPipeline-snakemake/SGE_cluster.json --keep-going --cluster "qsub -N {config[cohort_name]}_{rule} -V -cwd -m ea -M {cluster.user_mail} -pe {cluster.parall_env} {threads} -o {config[paths][log_folder]}/\$JOB_ID_{config[cohort_name]}_{rule}.log -e {config[paths][log_folder]}/\$JOB_ID_{config[cohort_name]}_{rule}.e -V -l h_vmem={cluster.mem} -q {cluster.queue}"
# echo "snakemake -s ~/scripts/pipelines/ImputationPipeline-snakemake/Snakefile -p -r --configfile /home/cocca/analyses/test_imputation_20210604/config_test_2.yaml --cores 30 --keep-going"
# echo "snakemake -s ~/scripts/pipelines/ImputationPipeline-snakemake/Snakefile -p -r --configfile /home/cocca/analyses/test_imputation/config_test_2.yaml --cores 32 " | qsub -N test_imputation -o ./
```

---
#3/6/2021

We needed to create a resource for allele check and alignment.
We need to use opnl
We used version 154 of dbSNP.

```bash
for chr in {1..22} X Y MT
do
echo "bcftools view -m2 -M2 /netapp/nfs/resources/dbSNP/human_9606_b154_GRCh37p13/VCF/GCF_000001405.25.chr${chr}.vcf.gz |bcftools query -f'%CHROM\t%POS\t%ID\t%REF\t%ALT\n' > /netapp/nfs/resources/dbSNP/human_9606_b154_GRCh37p13/VCF/GCF_000001405.25.chr${chr}.dbSNP154.tab" | qsub -N get_${chr}_data -V -cwd -l h_vmem=15G -q fast
done
```

Merge in a single table

```bash
(for chr in {1..22} X Y MT
do
cat /netapp/nfs/resources/dbSNP/human_9606_b154_GRCh37p13/VCF/GCF_000001405.25.chr${chr}.dbSNP154.tab
done) > /netapp/nfs/resources/dbSNP/human_9606_b154_GRCh37p13/GCF_000001405.25.dbSNP154.tab
```

---
#4/6/2021

The first step of a2 update, gives 47 warning for "Impossible A2 allele assignment".
Working in folder ~/analyses/test_imputation/00.splitted_input.

```bash
fgrep "Impossible A2 allele assignment" *.log |cut -f 8 -d " "|sed 's/\.//g'
```

We want to see what kind of variants are those, in dbSNP data and in our data.

```bash
fgrep -w -f <(fgrep "Impossible A2 allele assignment" *.log |cut -f 8 -d " "|sed 's/\.//g') /netapp/nfs/resources/dbSNP/human_9606_b154_GRCh37p13/GCF_000001405.25.dbSNP154.tab > ../impossible_a2_assign_dbSNP.tab
fgrep -w -f <(fgrep "Impossible A2 allele assignment" *.log |cut -f 8 -d " "|sed 's/\.//g') Slo_POP_snps_only_mapUpdateExt.bim
```

We can see that they are all flipped snps in the snp array dataset. So, actually, it is ok.

We also have 458 warnings for "Impossible A1 allele assignment"

```bash
fgrep "Impossible A1 allele assignment" *.log |cut -f 8 -d " "|sed 's/\.//g'
```

We want to see what kind of variants are those, in dbSNP data and in our data.

```bash
fgrep -w -f <(fgrep "Impossible A1 allele assignment" *.log |cut -f 8 -d " "|sed 's/\.//g') /netapp/nfs/resources/dbSNP/human_9606_b154_GRCh37p13/GCF_000001405.25.dbSNP154.tab > ../impossible_a1_assign_dbSNP.tab
fgrep -w -f <(fgrep "Impossible A1 allele assignment" *.log |cut -f 8 -d " "|sed 's/\.//g') *.bim
```


In the menawhile, we need to rebuild the ref table with dbSNP, removing INDELs

```bash
for chr in {1..22} X Y MT
do
echo "bcftools view -m2 -M2 -v snps /netapp/nfs/resources/dbSNP/human_9606_b154_GRCh37p13/VCF/GCF_000001405.25.chr${chr}.vcf.gz |bcftools query -f'%CHROM\t%POS\t%ID\t%REF\t%ALT\n' > /netapp/nfs/resources/dbSNP/human_9606_b154_GRCh37p13/VCF/GCF_000001405.25.chr${chr}.dbSNP154.tab" | qsub -N get_${chr}_data -V -cwd -l h_vmem=15G -q fast
done
```

and merge all back in a single table

```bash
(for chr in {1..22} X
do
cat /netapp/nfs/resources/dbSNP/human_9606_b154_GRCh37p13/VCF/GCF_000001405.25.chr${chr}.dbSNP154.tab
done) > /netapp/nfs/resources/dbSNP/human_9606_b154_GRCh37p13/GCF_000001405.25.SNPS.dbSNP154.tab
```

One problem with the allele update seems to be that we first updated a2, than a1. We tried also reversing the update, a1 first, than a2. This will still include errors due to the presence of INDELs in the reference file from dbSNP, but we'll fix that later.

We have 40 warnings for "Impossible A1 allele assignment"

```bash
fgrep "Impossible A1 allele assignment" *.log |cut -f 8 -d " "|sed 's/\.//g'
```

We want to see what kind of variants are those, in dbSNP data and in our data.

```bash
fgrep -w -f <(fgrep "Impossible A1 allele assignment" *.log |cut -f 8 -d " "|sed 's/\.//g') /netapp/nfs/resources/dbSNP/human_9606_b154_GRCh37p13/GCF_000001405.25.SNPS.dbSNP154.tab > ../impossible_a1_assign_dbSNP.tab
fgrep -w -f <(fgrep "Impossible A1 allele assignment" *.log |cut -f 8 -d " "|sed 's/\.//g') *.bim
```

We have 409 warnings for "Impossible A2 allele assignment"

```bash
fgrep "Impossible A2 allele assignment" *.log |cut -f 8 -d " "|sed 's/\.//g'
```

We want to see what kind of variants are those, in dbSNP data and in our data.

```bash
fgrep -w -f <(fgrep "Impossible A2 allele assignment" *.log |cut -f 8 -d " "|sed 's/\.//g') /netapp/nfs/resources/dbSNP/human_9606_b154_GRCh37p13/GCF_000001405.25.dbSNP154.tab > ../impossible_a2_assign_dbSNP.tab
fgrep -w -f <(fgrep "Impossible A2 allele assignment" *.log |cut -f 8 -d " "|sed 's/\.//g') Slo_POP_snps_only_mapUpdateExt.bim
```

So, the most likely reason for the impossible allele assignment is a flipping problem. 
We will than use the first rule to get all the unassigned snps, flip them in the original dataset and rerun the rule.
This way, we could still have some unassigned variant, but those should be only due to actual mismatch with the reference data.

rs375065198
rs199994882


#05/06/2020
Sample command to run the pipeline on system queues

```bash
snakemake -s ~/scripts/pipelines/ImputationPipeline-snakemake/Snakefile -p -r --jobs 50 --configfile /home/cocca/analyses/test_imputation_20210604/config_test_2.yaml --cluster-config ~/scripts/pipelines/ImputationPipeline-snakemake/SGE_cluster.json --keep-going --cluster "qsub -N {config[cohort_name]}_{rule} -V -cwd -m ea -M {cluster.user_mail} -pe {cluster.parall_env} {threads} -o {config[paths][log_folder]}/\\$JOB_ID_{config[cohort_name]}_{rule}.log -e {config[paths][log_folder]}/\\$JOB_ID_{config[cohort_name]}_{rule}.e -V -l h_vmem={cluster.mem} -q {cluster.queue}"
```

---
#6/6/2021

Added rules to retrieve monomorphic sites.

We need also to update the imputation rule to use IMPUTE5.

We need to generate a new set of reference panels, using custom IMPUTE5 format.
First, convert to VCF format:

```bash
basepath=/shared/resources/references/IGRPv1
for chr in {1..22}
do

mkdir -p ${basepath}/VCF/${chr}

echo "bcftools convert --haplegendsample2vcf ${basepath}/HAP_LEGEND_SAMPLES/${chr}/${chr}.IGRPv1.hap.gz,${basepath}/HAP_LEGEND_SAMPLES/${chr}/${chr}.IGRPv1.legend.gz,${basepath}/HAP_LEGEND_SAMPLES/${chr}/${chr}.IGRPv1.samples -O z -o ${basepath}/VCF/${chr}/${chr}.IGRPv1.vcf.gz;tabix -p vcf ${basepath}/VCF/${chr}/${chr}.IGRPv1.vcf.gz" |qsub -N convert_${chr}_vcf -V -cwd -l h_vmem=15G -q fast

done
```

We need to clean the VCF files from variants other than SNPs and Indels, since it seems there are issues with characters other than ATCG in ALT field (<CN0>, <INS:...:...>, etc)

```bash
basepath=/shared/resources/references/IGRPv1
for chr in {1..21}
do

mkdir -p ${basepath}/VCF/${chr}

echo "bcftools view -e\"ALT~'<'\" ${basepath}/VCF/${chr}/${chr}.IGRPv1.vcf.gz -O z -o ${basepath}/VCF/${chr}/${chr}.IGRPv1.clean.vcf.gz;tabix -p vcf ${basepath}/VCF/${chr}/${chr}.IGRPv1.clean.vcf.gz" |qsub -N clean_${chr}_vcf -V -cwd -l h_vmem=15G -q fast
done
```


After this first conversion and the cleaning, we can convert all to IMP5 format using the converter by Marchini et al.

```bash
basepath=/shared/resources/references/IGRPv1
# for chr in {7..21}
# for chr in {1..22}
for chr in 22
do
mkdir -p ${basepath}/IMP5/${chr}
# echo "/shared/software/impute5_v1.1.4/imp5Converter_1.1.4_static --h ${basepath}/VCF/${chr}/${chr}.IGRPv1.vcf.gz --r ${chr} --o ${basepath}/IMP5/${chr}/${chr}.IGRPv1.imp5" |qsub -N convert_${chr}_imp5 -V -cwd -l h_vmem=15G -q fast
# echo "/shared/software/impute5_v1.1.5/imp5Converter_1.1.5_static --h ${basepath}/VCF/${chr}/${chr}.IGRPv1.vcf.gz --r ${chr} --o ${basepath}/IMP5/${chr}/${chr}.IGRPv1.imp5" |qsub -N convert_${chr}_imp5 -V -cwd -l h_vmem=15G -q fast
# echo "/shared/software/impute5_v1.1.5/imp5Converter_1.1.5_static --h ${basepath}/VCF/${chr}/${chr}.IGRPv1.clean.vcf.gz --r ${chr} --o ${basepath}/IMP5/${chr}/${chr}.IGRPv1.imp5" |qsub -N convert_${chr}_imp5 -V -cwd -l h_vmem=15G -q fast
echo "/shared/software/impute5_v1.1.5/imp5Chunker_1.1.5_static --h ${basepath}/VCF/${chr}/${chr}.IGRPv1.clean.vcf.gz --r ${chr} --g /home/cocca/analyses/test_imputation_20210604/04.phased_data/IGRPv1/Slo_POP_22_phased.vcf.gz --o /home/cocca/analyses/test_imputation_20210604/04.phased_data/IGRPv1/Slo_POP_22_phased.coordinates.txt" |qsub -N chunk_${chr}_imp5 -V -cwd -l h_vmem=15G -q fast
done
```


We need to add rsIDs to ref panel in VCF format:

```bash
basepath=/shared/resources/references/VCF/IGRPv1
dbSNP=/netapp/nfs/resources/dbSNP/human_9606_b154_GRCh37p13/GCF_000001405.25.vcf.gz
# for chr in 1 {3..22}
# for chr in 2
for chr in 1 3 4
do

mkdir -p ${basepath}/a_rsID/${chr}

# echo "bcftools annotate --set-id +'%CHROM:%POS\_%REF\_%FIRST_ALT' ${basepath}/noRs/${chr}/${chr}.IGRPv1.vcf.gz -O z -o ${basepath}/${chr}/${chr}.IGRPv1.vcf.gz;tabix -p vcf ${basepath}/${chr}/${chr}.IGRPv1.vcf.gz" |qsub -N annots_${chr}_vcf -V -cwd -l h_vmem=20G -q fast
echo "bcftools annotate -a ${dbSNP} -c ID ${basepath}/${chr}/${chr}.IGRPv1.vcf.gz -O z -o ${basepath}/a_rsID/${chr}/${chr}.IGRPv1.vcf.gz;tabix -p vcf ${basepath}/a_rsID/${chr}/${chr}.IGRPv1.vcf.gz" |qsub -N annots_${chr}_vcf -V -cwd -l h_vmem=15G -q "fast@apollo2.burlo.trieste.it"

done
```

Test fixref plugin

```bash
bcftools +fixref /home/cocca/analyses/test_imputation_20210604/03.flipped_input/IGRPv1/VCF/Slo_POP_22_allFix_flipped.vcf.gz -O z -o /home/cocca/analyses/test_imputation_20210604/03.flipped_input/IGRPv1/VCF/Slo_POP_22_allFix_flipped_REFFIX.vcf.gz -- -f /shared/resources/hgRef/hg19/hg19_nochr.fasta -i /netapp/nfs/resources/dbSNP/human_9606_b154_GRCh37p13/GCF_000001405.25.vcf.gz
```
---
#12/06/2021

Sample command to run the pipeline on system queues

```bash
snakemake -s ~/scripts/pipelines/ImputationPipeline-snakemake/Snakefile --restart-times 3 -p -r --jobs 100 --configfile /home/cocca/analyses/test_imputation_20210604/config_test_2.yaml --cluster-config ~/scripts/pipelines/ImputationPipeline-snakemake/SGE_cluster.json --cluster "qsub -N {config[cohort_name]}_{rule} -V -cwd -m ea -M {cluster.user_mail} -pe {cluster.parall_env} {threads} -o {log.stdout} -e {log.stderr} -l h_vmem={cluster.mem} -q {cluster.queue}"
```


Try to work with profiles to submit jobs on cluster, following the new snakemake standard, since --clustrer-config is deprecated (though still working)

```bash
snakemake -s ~/scripts/pipelines/ImputationPipeline-snakemake/Snakefile -p -r --configfile /home/cocca/analyses/test_imputation_20210604/config_test_2.yaml --profile ~/scripts/pipelines/ImputationPipeline-snakemake/profiles/SGE
```

---
#25/6/2021

Added pylatex and texlive-core to the installed packages in the conda env to generate the imputation report

---

#29/10/2021

Convert also chrX from 1000G to the correct format, splitting par and non par regions

```bash
basepath=/shared/resources/references

infile=/netapp/nfs/resources/1000GP_phase3/vcf/ALL.chrX.phase3_shapeit2_mvncall_integrated_v1b.20130502.genotypes.vcf.gz
chr="X"

mkdir -p ${basepath}/IMP5/TGP3/${chr}_NONPAR
mkdir -p ${basepath}/IMP5/TGP3/${chr}_PAR1
mkdir -p ${basepath}/IMP5/TGP3/${chr}_PAR2
# echo "/shared/software/impute5_v1.1.4/imp5Converter_1.1.4_static --h ${basepath}/VCF/${chr}/${chr}.IGRPv1.vcf.gz --r ${chr} --o ${basepath}/IMP5/${chr}/${chr}.IGRPv1.imp5" |qsub -N convert_${chr}_imp5 -V -cwd -l h_vmem=15G -q fast
# echo "/shared/software/impute5_v1.1.5/imp5Converter_1.1.5_static --h ${basepath}/VCF/${chr}/${chr}.IGRPv1.vcf.gz --r ${chr} --o ${basepath}/IMP5/${chr}/${chr}.IGRPv1.imp5" |qsub -N convert_${chr}_imp5 -V -cwd -l h_vmem=15G -q fast
echo "/shared/software/impute5_v1.1.5/imp5Converter_1.1.5_static --h ${infile} --r ${chr}:2699521-154931043 --o ${basepath}/IMP5/TGP3/${chr}_NONPAR/${chr}_NONPAR.TGP3.imp5" |qsub -N convert_${chr}_NONPAR_imp5 -V -cwd -l h_vmem=15G -q all.q
echo "/shared/software/impute5_v1.1.5/imp5Converter_1.1.5_static --h ${infile} --r ${chr}:60001-2699520 --o ${basepath}/IMP5/TGP3/${chr}_PAR1/${chr}_PAR1.TGP3.imp5" |qsub -N convert_${chr}_PAR1_imp5 -V -cwd -l h_vmem=15G -q all.q
echo "/shared/software/impute5_v1.1.5/imp5Converter_1.1.5_static --h ${infile} --r ${chr}:154931044-155260560 --o ${basepath}/IMP5/TGP3/${chr}_PAR2/${chr}_PAR2.TGP3.imp5" |qsub -N convert_${chr}_PAR2_imp5 -V -cwd -l h_vmem=15G -q all.q

basepath=/shared/resources/references
chr="X"
invcf=/netapp/nfs/resources/1000GP_phase3/vcf/ALL.chrX.phase3_shapeit2_mvncall_integrated_v1b.20130502.genotypes.vcf.gz
echo "bcftools view -r ${chr}:2699521-154931043 ${invcf} -O z -o ${basepath}/VCF/TGP3/${chr}/${chr}.TGP3.vcf.gz" |qsub -N convert_${chr}_NONPAR -V -cwd -l h_vmem=15G -q all.q
echo "bcftools view -r ${chr}:60001-2699520 ${invcf} -O z -o ${basepath}/VCF/TGP3/${chr}_par1/${chr}_par1.TGP3.vcf.gz" |qsub -N convert_${chr}_PAR1 -V -cwd -l h_vmem=15G -q all.q
echo "bcftools view -r ${chr}:154931044-155260560 ${invcf} -O z -o ${basepath}/VCF/TGP3/${chr}_par2/${chr}_par2.TGP3.vcf.gz" |qsub -N convert_${chr}_PAR2 -V -cwd -l h_vmem=15G -q all.q

# chrX:60001-2699520 and chrX:154931044-155260560 -> PAR regions for b37
```


---
#15/11/2021

We need to take care consistently of the X chromosome for the imputation

We will need the sex information, since we are going to split our dataset in MALES and FEMALES, to perform phasing and imputation. Phasing will not be performed for hapoid males samples.

We need to check the sex cromosome coding in plink files. The genotyping QC pipeline we created should produce a set of genotypes including sex chromosomes:

* chrX -> 23
* chrY -> 24
* chrM -> 26

We shouldn't have any 25, in the coding, meaning that the chr X data is merged.
We will not impute chrY and chrM.

We will need to split the data for par and nonpar regions, at some point.

Since in all the reference data used chrX is coded as X and not as 23, we will need to perform a fix on the input file, as a first rule, so we can have a consistent naming.


```bash
infile=/home/cocca/analyses/imputation/20211116/00.cleaned_input/MOLISANI_chrXrenamed
outfile=/home/cocca/analyses/imputation/20211116/00.cleaned_input/MOLISANI_chrXsplitted
plink --file ${infile} --split-x hg19 'no-fail' --recode --out ${outfile}

infile=/home/cocca/analyses/imputation/20211116/00.cleaned_input/MOLISANI_chrXrenamed
outfile=/home/cocca/analyses/imputation/20211116/00.cleaned_input/MOLISANI_snps_only

plink --file ${infile} --snps-only 'just-acgt' --recode --out ${outfile}



snakemake -s ~/scripts/pipelines/ImputationPipeline-snakemake/Snakefile -n -p -r --jobs 100 --configfile /home/cocca/analyses/imputation/20211116_TEST/test_imputation_20211116.yaml --omit-from vcfAnnotate --cluster-config ~/scripts/pipelines/ImputationPipeline-snakemake/SGE_cluster.json --cluster "qsub -N {config[cohort_name]}_{rule} -V -cwd -m ea -M {cluster.user_mail} -pe {cluster.parall_env} {threads} -o {log.stdout} -e {log.stderr} -l h_vmem={cluster.mem} -q {cluster.queue}"

snakemake -s ~/scripts/pipelines/ImputationPipeline-snakemake/Snakefile -n -p -r --jobs 100 --configfile /home/cocca/analyses/imputation/20211116_TEST/test_imputation_20211116.yaml --omit-from vcfAnnotate 
```

---
#23/11/2021

Fixing the genotype input file, it seems is not feasible, since plink keeps reverting the chromosome coding from chrX to 23 and 25.
So we will need to create the reference panels and relevant files accordingly. It will be enough to generate input files with the correct naming.

It is not the best choice, IMHO, but we can try merging the par regions for the hap/legend/samples

```bash
cat 1000GP_Phase3_chrX_PAR1.hap.gz 1000GP_Phase3_chrX_PAR2.hap.gz > 1000GP_Phase3_25.hap.gz
cat 1000GP_Phase3_chrX_PAR1.legend.gz 1000GP_Phase3_chrX_PAR2.legend.gz > 1000GP_Phase3_25.legend.gz

```


Test run for the preproc set of rules

```bash
basefolder=/home/cocca/analyses/imputation/20211116_TEST
err_name=${basefolder}/Logs/test_chrX_imputation.err
stdout_name=${basefolder}/Logs/test_chrX_imputation.log
config_file=/home/cocca/analyses/imputation/20211116_TEST/test_imputation_20211116.yaml

mkdir -p ${basefolder}/Logs

# snakemake -s ~/scripts/pipelines/ImputationPipeline-snakemake/Snakefile -p -r --jobs 100 --configfile ${config_file} --omit-from vcfAnnotate --keep-going --cluster-config ~/scripts/pipelines/ImputationPipeline-snakemake/SGE_cluster.json --cluster "qsub -N {rule}_{config[cohort_name]} -V -cwd -m ea -M {cluster.user_mail} -pe {cluster.parall_env} {threads} -o {log.stdout} -e {log.stderr} -l h_vmem={cluster.mem} -q {cluster.queue}" 1> ${stdout_name} 2> ${err_name}
# snakemake -s ~/scripts/pipelines/ImputationPipeline-snakemake/Snakefile -p -r --jobs 100 --configfile ${config_file} --omit-from phase --keep-going --cluster-config ~/scripts/pipelines/ImputationPipeline-snakemake/SGE_cluster.json --cluster "qsub -N {rule}_{config[cohort_name]} -V -cwd -m ea -M {cluster.user_mail} -pe {cluster.parall_env} {threads} -o {log.stdout} -e {log.stderr} -l h_vmem={cluster.mem} -q {cluster.queue}" 1> ${stdout_name} 2> ${err_name}
# snakemake -s ~/scripts/pipelines/ImputationPipeline-snakemake/Snakefile -p -r --jobs 100 --configfile ${config_file} --omit-from chunkGenerator --keep-going --cluster-config ~/scripts/pipelines/ImputationPipeline-snakemake/SGE_cluster.json --cluster "qsub -N {rule}_{config[cohort_name]} -V -cwd -m ea -M {cluster.user_mail} -pe {cluster.parall_env} {threads} -o {log.stdout} -e {log.stderr} -l h_vmem={cluster.mem} -q {cluster.queue}" 1> ${stdout_name} 2> ${err_name}
snakemake -s ~/scripts/pipelines/ImputationPipeline-snakemake/Snakefile -n -p -r --jobs 100 --configfile ${config_file} --omit-from impute --keep-going

```


---
#3/12/2121

We have another problem, in the recoverMono rule. If we use a SNP array bigger than 600K, it takes forever to finish this step.
We can split in chunks and parallelize this, and we should get a boost in performances, since we have to use a reference file for allele update that is not splittable by chromosome.

Cehck prefix/suffix for splitted files 

Test new committed rules/fixes, on recmonfix test branch

```bash
source activate snakemake_g

basefolder=/home/cocca/analyses/imputation/20211116_TEST
err_name=${basefolder}/Logs/test_BIMSPLIT_imputation_20211203_1.err
stdout_name=${basefolder}/Logs/test_BIMSPLIT_imputation_20211203_1.log
config_file=/home/cocca/analyses/imputation/20211116_TEST/test_imputation_20211116.yaml

mkdir -p ${basefolder}/Logs

snakemake -s /home/cocca/scripts/pipelines/test_branches/ImputationPipeline-snakemake/Snakefile -p -r --jobs 100 --configfile ${config_file} --omit-from vcfFixRef --keep-going --cluster-config ~/scripts/pipelines/ImputationPipeline-snakemake/SGE_cluster.json --cluster "qsub -N {rule}_{config[cohort_name]} -V -cwd -m ea -M {cluster.user_mail} -pe {cluster.parall_env} {threads} -o {log.stdout} -e {log.stderr} -l h_vmem={cluster.mem} -q {cluster.queue}" 1> ${stdout_name} 2> ${err_name}

```

---

#15/12/2021

We need to fix the aggregator rule to generate a single pdf for at-a-glance QC of each chromosome

Test the error we were getting on one test dataset

```bash
source activate snakemake_g

basefolder=/home/cocca/analyses/imputation/20211215_TEST
err_name=${basefolder}/Logs/test_collstats_imputation_20211215_1.err
stdout_name=${basefolder}/Logs/test_collstats_imputation_20211215_1.log
config_file=/home/cocca/analyses/imputation/20211215_TEST/test_collstats_20211215.yml

mkdir -p ${basefolder}/Logs

# snakemake -n -s /home/cocca/scripts/pipelines/test_branches/ImputationPipeline-snakemake/Snakefile -p -r --jobs 100 --configfile ${config_file} --keep-going 
snakemake -s /home/cocca/scripts/pipelines/test_branches/ImputationPipeline-snakemake/Snakefile -p -r --jobs 100 --configfile ${config_file} --keep-going --cluster-config ~/scripts/pipelines/ImputationPipeline-snakemake/SGE_cluster.json --cluster "qsub -N {rule}_{config[cohort_name]} -V -cwd -pe {cluster.parall_env} {threads} -o {log.stdout} -e {log.stderr} -l h_vmem={cluster.mem} -q {cluster.queue}" 1> ${stdout_name} 2> ${err_name}

```

For future reference, here is the latency error we get on netapp05

```
Submitted job 510 with external jobid 'Your job 503352 ("infoStatsChunks_NUTRIACT") has been submitted'.
MissingOutputException in line 2 of /home/cocca/scripts/pipelines/ImputationPipeline-snakemake/rules/postproc.smk:
Job Missing files after 40 seconds:
/netapp05/imputation/IGRPv1/NUTRIACT/07.stats/7/CHUNKS/7_08_impute_summary_by_maf_by_info.csv
/netapp05/imputation/IGRPv1/NUTRIACT/07.stats/7/CHUNKS/7_08_impute_summary_by_maf.csv
/netapp05/imputation/IGRPv1/NUTRIACT/07.stats/7/CHUNKS/7_08_impute_summary.png
/netapp05/imputation/IGRPv1/NUTRIACT/07.stats/7/CHUNKS/7_08_impute_manhattan.png
This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait.
Job id: 934 completed successfully, but some output files are missing. 934
  File "/shared/software/conda/envs/snakemake_g/lib/python3.9/site-packages/snakemake/executors/__init__.py", line 824, in handle_job_success
  File "/shared/software/conda/envs/snakemake_g/lib/python3.9/site-packages/snakemake/executors/__init__.py", line 252, in handle_job_success
Job failed, going on with independent jobs.
```


X.TGP3.vcf.gz


155260560



```bash
source activate snakemake_g

basefolder=/home/cocca/analyses/imputation/20220203_TEST
err_name=${basefolder}/Logs/test_collstats_imputation_20211215_1.err
stdout_name=${basefolder}/Logs/test_collstats_imputation_20211215_1.log
config_file=/home/cocca/analyses/imputation/20220203_TEST/test_collstats_20220203.yml

mkdir -p ${basefolder}/Logs

# snakemake -n -s /home/cocca/scripts/pipelines/test_branches/ImputationPipeline-snakemake/Snakefile -p -r --jobs 100 --configfile ${config_file} --keep-going 
snakemake -s /home/cocca/scripts/pipelines/test_branches/ImputationPipeline-snakemake/Snakefile -p -r --jobs 100 --configfile ${config_file} --keep-going --cluster-config ~/scripts/pipelines/ImputationPipeline-snakemake/SGE_cluster.json --cluster "qsub -N {rule}_{config[cohort_name]} -V -cwd -pe {cluster.parall_env} {threads} -o {log.stdout} -e {log.stderr} -l h_vmem={cluster.mem} -q {cluster.queue}" 1> ${stdout_name} 2> ${err_name}

```